# Задание 2 — Оптимизация доступа к памяти на GPU (CUDA)

## 1. Цель задания

Нужно реализовать CUDA-программу, которая показывает, как **паттерн доступа к глобальной памяти** влияет на производительность GPU.

По требованиям нужно:

1) сделать **две версии ядра**:
- (a) с **коалесцированным** (эффективным) доступом к global memory;
- (b) с **некоалесцированным** (неэффективным) доступом;

2) измерить время выполнения через **`cudaEvent`**;

3) провести оптимизацию за счёт:
- (a) **использования shared memory**,
- (b) **изменения организации потоков**;

4) сравнить результаты и сделать выводы.

---

## 2. Идея решения (что сделано в коде)

Программа обрабатывает массив размера `N = 1_000_000` и для каждой версии ядра выполняет одну и ту же операцию: умножение на 2.

Сделано **3 ядра**:

1) **Коалесцированный доступ** — `coalesced_kernel`  
Поток с индексом `idx` читает `in[idx]` и пишет `out[idx]`.  
Это самый “правильный” случай: потоки варпа обращаются к соседним адресам памяти → контроллер памяти может объединять обращения в крупные транзакции.

2) **Некоалесцированный доступ** — `noncoalesced_kernel`  
Индекс вычисляется как `idx = (global_tid) * 2`.  
В результате потоки идут по памяти “через элемент” и варп обращается к более разрозненным адресам → ухудшается объединение транзакций памяти, растут простои и падает пропускная способность.

3) **Версия с shared memory** — `shared_kernel`  
Каждый блок сначала загружает свой “тайл” в shared память: `tile[tid] = in[gid]`, затем из shared пишет результат: `out[gid] = tile[tid] * 2`.  
Цель — показать подход оптимизации через shared memory (хотя для простой операции `x*2` shared не всегда даёт выигрыш, зато хорошо демонстрирует механизм).

---

## 3. Что такое коалесцирование и почему это важно

### Коалесцированный доступ
Это когда потоки внутри варпа (обычно 32 потока) обращаются к **последовательным адресам** глобальной памяти.

Пример “правильного” доступа:
- поток 0 читает `in[0]`
- поток 1 читает `in[1]`
- поток 2 читает `in[2]`
- ...

GPU может обслужить такие чтения меньшим числом транзакций → выше пропускная способность.

### Некоалесцированный доступ
Это когда потоки обращаются к памяти “скачками” (stride) и адреса распределены хуже.

Пример “плохого” доступа:
- поток 0 читает `in[0]`
- поток 1 читает `in[2]`
- поток 2 читает `in[4]`
- ...

Варп не может эффективно “объединить” запросы → больше транзакций памяти → ниже производительность.

---

## 4. Как измеряется время (cudaEvent)

В коде есть функция замера `measure(...)`.

Логика такая:

- создаются события `start` и `stop` через `cudaEventCreate`
- делается прогрев ядра (warm-up), чтобы исключить “первый запуск” и JIT-эффекты
- затем:
  - `cudaEventRecord(start)`
  - запуск ядра
  - `cudaEventRecord(stop)`
  - `cudaEventSynchronize(stop)`
  - `cudaEventElapsedTime(&ms, start, stop)`

То есть измеряется **время выполнения kernel** (одного запуска), а не копирования `H2D/D2H`.

---

## 5. Оптимизация: что именно считается “оптимизацией” в этой работе

### 5.1 Оптимизация через shared memory
В `shared_kernel` используется shared память блока:

- `__shared__ float tile[256];`
- чтение из global → запись в shared
- синхронизация `__syncthreads()`
- работа с shared → запись в global

Shared memory намного быстрее глобальной, но важно понимать:
- shared полезна, когда данные **переиспользуются** (например, свёртки, матричные операции, редукции),
- если элемент используется ровно 1 раз (как в `x*2`), shared может дать мало пользы или даже небольшой оверхед из-за копирования и синхронизации.

Зато этот вариант демонстрирует правильный шаблон: “global → shared → вычисление → global”.

### 5.2 Оптимизация через организацию потоков
Организация потоков в коде выбрана стандартная:

- `BLOCK = 256`
- `grid = (N + BLOCK - 1) / BLOCK`

Почему это адекватно:
- 256 — кратно размеру варпа (32), значит варпы формируются без “дырок”,
- достаточно потоков, чтобы GPU мог хорошо скрывать задержки памяти.

Для сравнения паттернов доступа важно, чтобы сетка/блоки были одинаковыми, иначе сравнение будет нечестным.

---

## 6. Что выводит программа

Программа выводит **время выполнения (мс)** для трёх вариантов:

- `Coalesced access time (ms)`
- `Non-coalesced access time (ms)`
- `Shared memory time (ms)`

---

## 7. Как интерпретировать результаты

При корректной реализации обычно наблюдается:

- **coalesced** быстрее (лучше пропускная способность памяти),
- **non-coalesced** медленнее (больше транзакций, больше простаивания),
- **shared** может быть:
  - быстрее, если shared реально уменьшает глобальные обращения или есть переиспользование,
  - или сопоставимо/хуже для простой операции из-за дополнительной стадии копирования и `__syncthreads()`.

Ключевой вывод задания: **паттерн доступа к глобальной памяти напрямую влияет на производительность**; плохие (stride/разрозненные) обращения резко увеличивают время выполнения kernel даже при одинаковой “арифметике”.
