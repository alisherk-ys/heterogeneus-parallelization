# Задание 3 — Профилирование гибридного приложения CPU + GPU

## 1. Цель задания

Нужно разработать **гибридную программу**, где часть вычислений выполняется на CPU, а часть — на GPU, и при этом:

1) реализовать гибридный алгоритм обработки массива;  
2) использовать **асинхронные копии** `cudaMemcpyAsync` и **CUDA streams**;  
3) выполнить профилирование:
   - определить накладные расходы передачи данных (Host↔Device),
   - выявить узкие места при взаимодействии CPU и GPU;  
4) предложить и реализовать **одну оптимизацию**, уменьшающую накладные расходы.

---

## 2. Что делает программа (операция)

Над массивом выполняется одинаковая операция:

- `out[i] = in[i] * k + 1`

Для проверки корректности заранее строится эталонный результат на CPU: `h_ref[i] = h_in[i] * k + 1`.

---

## 3. Идея гибридного решения

Массив длины `N` делится на две части:

- первая половина `[0 .. split-1]` обрабатывается на **CPU**,
- вторая половина `[split .. N-1]` обрабатывается на **GPU**.

Важно: CPU и GPU запускаются **одновременно**, чтобы добиться перекрытия (overlap) работы CPU и “пайплайна GPU” (копирование → kernel → копирование обратно).

В коде это реализовано так:

- CPU-часть запускается в отдельном потоке: `std::thread cpu_thr(...)`
- GPU-часть выполняется в отдельном CUDA stream: `cudaStreamCreate(&stream)`

---

## 4. Асинхронные копии и streams (почему это работает)

### 4.1 Почему нужен `cudaMemcpyAsync`
`cudaMemcpyAsync(...)` позволяет ставить копирование в очередь stream и **не блокировать** CPU на момент постановки операции. Это важно, чтобы CPU мог параллельно выполнять свою часть вычислений.

В программе GPU-часть — это цепочка операций в одном stream:

- Host→Device: `cudaMemcpyAsync(d_in, h_in + split, ...)`
- kernel: `process_kernel<<<..., stream>>>(...)`
- Device→Host: `cudaMemcpyAsync(h_out + split, d_out, ...)`

То есть stream формирует “конвейер” GPU.

### 4.2 Почему нужен pinned memory
Асинхронные копии Host↔Device корректно и эффективно работают, когда host-память закреплена (pinned / page-locked), поэтому используется:

- `cudaHostAlloc(&h_in, ...)`
- `cudaHostAlloc(&h_out, ...)`
- `cudaHostAlloc(&h_ref, ...)`

Это одно из ключевых условий задания: pinned memory + `cudaMemcpyAsync` + stream дают реальную возможность перекрывать CPU-работу и передачи данных.

---

## 5. Что именно профилируется и как

### 5.1 Профилирование на стороне GPU (cudaEvent)
Для измерения времени конкретных этапов на GPU используются CUDA события:

- `e_h2d_s`, `e_h2d_e` — время копирования Host→Device  
- `e_ker_s`, `e_ker_e` — время выполнения kernel  
- `e_d2h_s`, `e_d2h_e` — время копирования Device→Host  

События пишутся **в тот же stream**, что и операции. Это важно: тогда измеряется именно время внутри данного потока выполнения.

Примеры измеряемых участков (внутри stream):

- `cudaEventRecord(e_h2d_s, stream)` … `cudaEventRecord(e_h2d_e, stream)`
- `cudaEventRecord(e_ker_s, stream)` … `cudaEventRecord(e_ker_e, stream)`
- `cudaEventRecord(e_d2h_s, stream)` … `cudaEventRecord(e_d2h_e, stream)`

Далее вычисляется время в миллисекундах через:
- `cudaEventElapsedTime(&h2d_ms, e_h2d_s, e_h2d_e)` и аналогично для kernel/D2H.

### 5.2 Профилирование CPU и общего времени (std::chrono)
CPU-часть (включая ожидания) измеряется через `std::chrono`:

- `cpu_ms` — время выполнения участка, где запускается CPU-поток и затем `join()`
- `total_ms` — общее время гибридного алгоритма от запуска до конца (CPU+GPU вместе)

### 5.3 Как считается накладная стоимость передачи данных
Накладные расходы передачи данных считаются как сумма двух замеров:

- `transfer_ms = h2d_ms + d2h_ms`

Это и есть “стоимость взаимодействия Host↔Device” для GPU-части.

### 5.4 Как оценить перекрытие (overlap)
В коде вычисляется оценка перекрытия:

- `overlap_est = total_ms - max(cpu_ms, gpu_pipeline_ms)`

где:
- `gpu_pipeline_ms = h2d_ms + ker_ms + d2h_ms`

Идея простая: если CPU и GPU работали параллельно, то общее время ближе к максимуму из двух веток, а не к их сумме.

---

## 6. Узкие места (bottlenecks), которые выявляет профилирование

По измерениям можно понять, что тормозит сильнее:

- если `transfer_ms` сравнимо или больше `ker_ms`, значит программа упирается в **передачи данных**, а не в вычисления;
- если `ker_ms` доминирует, значит узкое место — **вычислительная часть kernel**;
- если `cpu_ms` сильно больше `gpu_pipeline_ms`, значит ограничение — **CPU часть** (и GPU простаивает);
- если `gpu_pipeline_ms` больше `cpu_ms`, то ограничение — **GPU ветка** (и CPU часть завершается раньше).

Именно это требуется по заданию: выявить проблемный компонент взаимодействия CPU/GPU.

---

## 7. Предложенная и реализованная оптимизация (уменьшение накладных расходов)

### Оптимизация: pinned host memory + асинхронные копии в stream
Чтобы уменьшить накладные расходы и обеспечить перекрытие, в программе применены:

- pinned memory через `cudaHostAlloc(...)`
- асинхронные копии `cudaMemcpyAsync(..., stream)`
- единый stream для “пайплайна” GPU

Почему это уменьшает overhead:
- pinned memory снижает стоимость и делает возможными реальные async-копии,
- async + stream позволяет перекрыть CPU-вычисления с H2D/kernel/D2H,
- в итоге `total_ms` уменьшается относительно варианта, где всё делается последовательно и синхронно.

---

## 8. Что выводит программа

Программа выводит:

- время CPU-части (ms),
- время H2D (ms),
- время kernel (ms),
- время D2H (ms),
- суммарные накладные расходы передачи `H2D + D2H` (ms),
- полное время GPU-пайплайна `H2D + kernel + D2H` (ms),
- общее время гибридного выполнения (ms),
- оценку перекрытия CPU и GPU (ms),
- максимальную абсолютную ошибку и статус корректности результата.

  - узкие места CPU/GPU: ✅ (сравнение `cpu_ms`, `gpu_pipeline_ms`, `total_ms`)
- оптимизация overhead: ✅ (pinned memory + async copies + overlap CPU/GPU)
