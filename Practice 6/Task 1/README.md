# Задача №1 — OpenCL: сложение векторов на CPU и GPU + сравнение производительности

## Цель задания

Нужно реализовать OpenCL-приложение, которое выполняет **поэлементное сложение двух массивов**:

- `C[i] = A[i] + B[i]`

и запустить его **на CPU и на GPU**, чтобы сравнить производительность.

Отчёт (и вывод программы) должен содержать:

- время выполнения на **CPU**,
- время выполнения на **GPU**,
- **график сравнения** (можно построить отдельно по полученным числам).


## Идея решения

Решение разделено на 2 файла:

- `kernel.cl` — OpenCL-ядро (функция, которая запускается параллельно),
- `main.cpp` — хост-программа на C++, которая:
  - находит платформу и устройство,
  - создаёт контекст и очередь,
  - компилирует ядро,
  - создаёт буферы и копирует данные,
  - запускает ядро,
  - измеряет время,
  - считывает результат и проверяет корректность.

Также сделано удобно для отчёта: программу можно запустить как **cpu** или **gpu** через аргумент командной строки.

---

## Реализация ядра (kernel.cl)

### Ядро `vector_add`
В ядре каждый work-item (параллельный поток) обрабатывает один индекс `id`:

- `id = get_global_id(0)`
- если `id < n`:
  - `C[id] = A[id] + B[id]`


## Реализация host-программы (main.cpp) — по шагам

### Шаг 1. Выбор устройства (CPU или GPU)
- По умолчанию выбирается `GPU`.
- Если запустить `./app cpu`, то будет выбран CPU.
- Если нужного типа устройства нет — включён fallback на другой тип (это удобно, чтобы программа всё равно запускалась).

### Шаг 2. Подготовка данных на CPU
- Создаются массивы `A`, `B`, `C` и `C_ref`.
- `A` и `B` заполняются случайными числами.
- `C_ref` считается на CPU:
  - `C_ref[i] = A[i] + B[i]`
Это нужно для проверки корректности результата OpenCL.

### Шаг 3. Поиск платформы и устройства OpenCL
Программа:
- получает список платформ `clGetPlatformIDs`,
- перебирает платформы и ищет устройство нужного типа `clGetDeviceIDs`,
- выбирает первое найденное устройство.

Также печатается имя устройства (`CL_DEVICE_NAME`) — удобно вставлять в отчёт.

### Шаг 4. Создание контекста и командной очереди
- создаётся контекст `clCreateContext`,
- создаётся очередь `clCreateCommandQueue` с флагом:
  - `CL_QUEUE_PROFILING_ENABLE`  
  Это важно, чтобы можно было получить **точное время ядра** через event profiling.

### Шаг 5. Загрузка и компиляция ядра
- читается файл `kernel.cl`,
- создаётся программа `clCreateProgramWithSource`,
- компилируется `clBuildProgram`,
- создаётся kernel `clCreateKernel("vector_add")`.

Если компиляция не удалась — программа выводит build log (это критично для диагностики).

### Шаг 6. Создание буферов и копирование данных
На стороне устройства создаются буферы:

- `bufA`, `bufB` (READ_ONLY)
- `bufC` (WRITE_ONLY)

Данные `A` и `B` копируются на устройство через `clEnqueueWriteBuffer`.

### Шаг 7. Установка аргументов и запуск ядра
Аргументы:
- `bufA`, `bufB`, `bufC`, `n`

Запуск:
- `global = n`
- `local = nullptr` (то есть runtime сам выберет local size)

Запуск ядра:
- `clEnqueueNDRangeKernel(...)` + получение события `evt`
- `clFinish(queue)` — ждём завершения, иначе замер времени будет некорректным.


## Измерение времени (CPU vs GPU)

В программе выводятся **2 времени**:

### 6.1 Total time (enqueue + finish)
Замер через `std::chrono`:

- включает постановку в очередь + ожидание завершения (`clFinish`)
- это более “прикладное” время исполнения команды.

### 6.2 Kernel time (event profiling)
Если профилирование доступно, берётся время именно ядра:

- `CL_PROFILING_COMMAND_START`
- `CL_PROFILING_COMMAND_END`

и переводится из наносекунд в миллисекунды.

Это более точный показатель **чистого времени выполнения kernel**.


## Проверка корректности

После завершения ядра:
- `clEnqueueReadBuffer` считывает `C` на CPU,
- считается максимальная ошибка:

`max_abs_err = max(|C[i] - C_ref[i]|)`

Если ошибка близка к 0, то вычисления корректны.

---

## Что выводит программа

- выбранное устройство (имя + тип CPU/GPU),
- `Max abs error`,
- `Total time (enqueue+finish) ms`,
- `Kernel time (event profiling) ms`,
- `Done.`
