# Задание 1 — CUDA: редукция (сумма массива) с использованием shared memory

## 1. Цель задания

Нужно реализовать CUDA-ядро для **редукции (суммирования элементов массива)** и при этом:

- использовать **разделяемую память (shared memory)** для ускорения,
- проверить корректность на тестовом массиве (сравнить с CPU),
- получить итоговую сумму массива.


## 2. Идея решения

Редукция суммы на GPU обычно делается в несколько этапов:

1) **Каждый блок** суммирует свою часть массива и записывает **частичную сумму блока** в отдельный массив `block_sums`.
2) Затем массив частичных сумм снова редуцируется тем же ядром.
3) Повторяем, пока не останется **один элемент** — это и есть итоговая сумма.

То есть редукция выполняется “в несколько проходов”, пока размер данных не станет 1.


## 3. Что делает CUDA-ядро (reduce_sum_kernel)

### 3.1 Чтение данных из global memory
Каждый блок обрабатывает участок длиной `2 * blockDim.x`.

Индекс первого элемента для потока:

- `i = blockIdx.x * blockDim.x * 2 + threadIdx.x`

Каждый поток загружает **до двух элементов**:

- `input[i]`
- `input[i + blockDim.x]`

Это сделано для того, чтобы:
- уменьшить число блоков,
- увеличить загрузку памяти (лучше использовать bandwidth),
- ускорить редукцию (каждый поток сразу делает “частичную сумму”).

### 3.2 Запись в shared memory
После чтения из global каждый поток кладёт свою частичную сумму в shared:

- `sdata[tid] = sum`

Затем `__syncthreads()` — чтобы все потоки завершили загрузку перед редукцией.

### 3.3 Редукция внутри блока (binary tree)
Дальше выполняется классическая “деревянная” редукция:

- на каждом шаге `s` уменьшается в 2 раза:
  - сначала складываются элементы на расстоянии `blockDim/2`,
  - потом `blockDim/4`,
  - и так далее до `1`.

Псевдологика:

- если `tid < s`, то:
  - `sdata[tid] += sdata[tid + s]`

После каждого шага — `__syncthreads()`, чтобы не было гонок.

### 3.4 Запись результата блока
После редукции:

- поток `tid == 0` записывает сумму блока в глобальный массив:

- `block_sums[blockIdx.x] = sdata[0]`

Таким образом, каждый блок превращается в одно число.


## 4. Многошаговая редукция (как получаем одну сумму)

В `main()` редукция запускается циклом `while(true)`:

- `cur_in` — текущий вход (сначала это `d_in`),
- `cur_out` — текущий выход (сначала `d_out`),
- `cur_n` — текущий размер входа.

На каждой итерации:

1) вычисляем количество блоков:
   - `cur_blocks = ceil(cur_n / (2*threads))`

2) запускаем ядро:
   - `reduce_sum_kernel<<<cur_blocks, threads, threads*sizeof(float)>>>(...)`

3) если `cur_blocks == 1`, значит:
   - итоговая сумма лежит в `cur_out[0]`
   - выходим из цикла

4) иначе:
   - новый вход = массив частичных сумм,
   - уменьшаем размер `cur_n = cur_blocks`,
   - выделяем новый буфер под следующий выход (следующий уровень редукции).


## 5. Измерение времени GPU

Время редукции измеряется с помощью **CUDA Events**:

- `cudaEventRecord(start)`
- выполняется вся многошаговая редукция в цикле
- `cudaEventRecord(stop)` + `cudaEventSynchronize(stop)`
- `cudaEventElapsedTime(&ms, start, stop)`

Так получается время выполнения редукции на GPU (в миллисекундах).


## 6. Проверка корректности (GPU vs CPU)

### CPU-эталон
На CPU сумма считается обычным циклом в `double`:

- `cpu_sum(...)`

`double` используется для большей точности (на CPU меньше накопление ошибок).

### GPU-результат
Итоговый результат — это `cur_out[0]` (float), он копируется на CPU и переводится в double.

Далее считаются ошибки:

- `Abs error = |CPU - GPU|`
- `Rel error = Abs error / (|CPU| + 1e-12)`

Ожидаемо:
- абсолютная ошибка маленькая,
- относительная ошибка тоже маленькая (обычно `~1e-6 ... 1e-4`), потому что GPU суммирует в `float` и порядок сложения отличается от CPU.


## 7. Что выводит программа

Программа печатает:

- размер массива `N`,
- сумму на CPU,
- сумму на GPU,
- абсолютную и относительную ошибку,
- время GPU-редукции (ms).
