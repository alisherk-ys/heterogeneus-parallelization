# Задание 2 — MPI: распределённое решение СЛАУ методом Гаусса

## 1. Цель задания

Нужно реализовать распределённую программу на **MPI**, которая решает систему линейных уравнений методом Гаусса:

- есть матрица коэффициентов `A` размера `N×N`,
- есть вектор правых частей `b` размера `N`,
- требуется найти вектор решения `x` из `A·x = b`.

По условию:

1. Процесс с `rank = 0` создаёт матрицу `A` и вектор `b`.
2. Строки матрицы распределяются между процессами (по строкам) через Scatter (в решении использован вариант `MPI_Scatterv`, чтобы корректно работать при любом числе процессов).
3. Выполняются шаги метода Гаусса:
   - **прямой ход** (forward elimination) выполняется распределённо: каждый процесс исключает элементы в своих строках;
   - **обратный ход** (back substitution) выполняется после сборки результатов на `rank = 0`.
4. Используется `MPI_Bcast` для рассылки текущей опорной строки (pivot row) всем процессам на каждом шаге.
5. Выводится решение системы.

## 2. Идея решения

Метод Гаусса решает систему в два этапа:

### 2.1 Прямой ход (приведение к верхнетреугольному виду)
Для каждого столбца `k`:

- выбирается опорная строка `k` (pivot row),
- она нормализуется (делится на ведущий элемент, чтобы он стал равен 1),
- из всех строк ниже вычитается `factor * pivot_row`, чтобы занулить элемент в столбце `k`.

В распределённом варианте важно:
- опорная строка находится у конкретного процесса (владельца этой строки),
- после нормализации эта строка рассылается всем процессам через `MPI_Bcast`,
- каждый процесс обновляет только свои локальные строки.

### 2.2 Обратный ход (получение решения)
После прямого хода матрица становится верхнетреугольной, и решение находится с конца:

- начиная с `x[N-1]` и двигаясь вверх до `x[0]`.

В этом решении обратный ход делается на `rank = 0`, поэтому сначала результаты прямого хода собираются на `rank = 0`.

## 3. Как устроены данные в программе

Вместо хранения `A` и `b` отдельно используется расширенная матрица:

- `[A|b]` размера `N × (N+1)`.

То есть каждая строка содержит `N` коэффициентов и последний элемент — соответствующий `b[i]`.

В коде это хранится в одном массиве `Ab` (плоский вектор), доступ по индексу как:

`Ab[row*(N+1) + col]`

## 4. Генерация системы уравнений на rank 0

Чтобы алгоритм работал устойчиво и не сталкивался часто с нулевыми pivot-элементами, на `rank = 0` создаётся **диагонально-доминантная** система:

- по каждой строке диагональный элемент увеличивается так, чтобы быть больше суммы модулей остальных элементов.

Это повышает вероятность, что:
- ведущие элементы (pivot) не будут близки к нулю,
- метод Гаусса будет работать стабильно без перестановок строк.

## 5. Распределение строк между процессами

Требование задания — корректная работа при любом количестве процессов.  
Поэтому строки делятся так, чтобы:

- каждый процесс получил либо `⌊N/size⌋`, либо `⌊N/size⌋ + 1` строку,
- первые процессы получают “лишние” строки (остаток `N % size`).

Для этого используются массивы:

- `counts[p]` — число строк у процесса `p`,
- `displs[p]` — глобальный индекс первой строки процесса `p`.

Чтобы разослать строки расширенной матрицы `[A|b]`, формируются:

- `sendcounts[p] = counts[p] * (N+1)`,
- `senddispls[p] = displs[p] * (N+1)`.

Распределение выполняется вызовом:
`MPI_Scatterv(...)`

(Это логически соответствует “Scatter строк”, но делает решение корректным при `N % size != 0`.)

## 6. Прямой ход (распределённо) + MPI_Bcast pivot-строки

На каждом шаге `k`:

1. Определяется процесс-владелец глобальной строки `k`.  
   Это делает функция `owner_of_row(...)`.

2. Процесс-владелец:
   - копирует свою строку `k` в буфер `pivot_row`,
   - нормализует её (делит на pivot-элемент),
   - обновляет эту же строку у себя локально.

3. Затем pivot-строка рассылается всем процессам:
   `MPI_Bcast(pivot_row.data(), N + 1, MPI_DOUBLE, owner, MPI_COMM_WORLD)`

4. Каждый процесс применяет исключение к своим строкам ниже `k`:
   - берёт `factor = Ab_local[row, k]`,
   - делает `row = row - factor * pivot_row`,
   - вручную ставит `Ab_local[row, k] = 0.0` для снижения численного “мусора”.

Таким образом достигается требование:
- “использовать `MPI_Bcast` для передачи текущей строки другим процессам во время прямого хода”.

## 7. Сбор результатов и обратный ход на rank 0

После окончания прямого хода каждый процесс хранит “свои” преобразованные строки.

Чтобы выполнить обратный ход на одном процессе:

- все локальные блоки собираются на `rank = 0` через `MPI_Gatherv(...)`.

После этого `rank = 0` выполняет обратную подстановку и получает вектор решения `x`.

## 8. Замер времени

Для оценки производительности прямого хода используется `MPI_Wtime()`:

- время берётся перед циклом прямого хода и после него,
- выводится только на `rank = 0`.

Это даёт время именно распределённой части вычислений (forward elimination).

## 9. Вывод программы

Программа выводит (на `rank = 0`):

- первые несколько значений вектора решения `x` (обычно первые 10),
- время выполнения прямого хода (forward elimination) по `MPI_Wtime`.
