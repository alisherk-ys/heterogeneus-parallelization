# Задание 1 — MPI: распределённое вычисление среднего и стандартного отклонения

## 1. Цель задания

Нужно реализовать распределённую программу на **MPI**, которая вычисляет для большого массива размера `N`:

- **среднее значение** (mean),
- **стандартное отклонение** (standard deviation).

По условию:

1. Массив случайных чисел создаётся только на процессе с `rank = 0`.
2. Массив распределяется между всеми процессами через `MPI_Scatterv` (так как `N` может не делиться нацело).
3. Каждый процесс считает локально:
   - сумму элементов своей части,
   - сумму квадратов элементов своей части.
4. Локальные суммы собираются на `rank = 0` через `MPI_Reduce`.
5. На `rank = 0` по глобальным суммам вычисляются mean и stddev.
6. Результаты выводятся на экран.

## 2. Идея решения

Основная идея — посчитать статистику через две агрегированные величины:

- `S = sum(x_i)`
- `Q = sum(x_i^2)`

Тогда:

- `mean = S / N`
- `variance = (Q / N) - mean^2`
- `stddev = sqrt(variance)`

В MPI это удобно, потому что суммы легко “склеиваются” через редукцию (сложение):

- каждый процесс считает свои `local_sum` и `local_sq_sum`,
- затем `MPI_Reduce` собирает `global_sum` и `global_sq_sum` на `rank = 0`.

## 3. Подготовка данных на rank 0

Только процесс `rank = 0`:

- выделяет полный массив `data` размера `N`,
- заполняет его случайными числами.

Дополнительно `rank = 0` готовит параметры для `MPI_Scatterv`:

- `counts[i]` — сколько элементов отправить процессу `i`,
- `displs[i]` — с какого индекса начинается кусок процесса `i`.

Так как `N` может не делиться на `size`, используется схема:

- `base = N / size`
- `remainder = N % size`
- первые `remainder` процессов получают на 1 элемент больше.

## 4. Почему используется MPI_Scatterv

Обычный `MPI_Scatter` требует, чтобы каждый процесс получил одинаковое количество элементов.

Но если `N` не делится на число процессов, то размеры частей будут разными.  
Поэтому используется `MPI_Scatterv`, который позволяет:

- отправлять разное количество элементов каждому процессу (`counts`),
- задавать смещения начала каждого блока (`displs`).

## 5. Как каждый процесс узнаёт размер своей части

До `MPI_Scatterv` каждый процесс должен знать, сколько элементов ему придёт, чтобы выделить буфер.

Для этого используется отдельная рассылка:

`MPI_Scatter(counts.data(), 1, MPI_INT, &local_n, 1, MPI_INT, 0, MPI_COMM_WORLD)`

После этого каждый процесс:

- получает `local_n`,
- выделяет `local_data(local_n)`.

## 6. Распределение массива между процессами

Распределение выполняется одной командой:

`MPI_Scatterv(data.data(), counts.data(), displs.data(), MPI_DOUBLE, local_data.data(), local_n, MPI_DOUBLE, 0, MPI_COMM_WORLD)`

Результат:
- каждый процесс получает только свой кусок массива в `local_data`.

## 7. Локальные вычисления на каждом процессе

Каждый процесс вычисляет две величины по своему куску:

- `local_sum = sum(local_data)`
- `local_sq_sum = sum(local_data[i]^2)`

Это делается обычным циклом, потому что объём данных на каждом процессе меньше общего массива.

## 8. Сбор глобальных сумм через MPI_Reduce

Далее все процессы отправляют свои локальные результаты в редукцию:

- `MPI_Reduce(&local_sum, &global_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD)`
- `MPI_Reduce(&local_sq_sum, &global_sq_sum, 1, MPI_DOUBLE, MPI_SUM, 0, MPI_COMM_WORLD)`

Что происходит:
- MPI суммирует значения со всех процессов,
- итоговые суммы сохраняются только у процесса `rank = 0`.

## 9. Финальные вычисления на rank 0

На `rank = 0` уже есть:

- `global_sum = sum(x_i)` по всему массиву,
- `global_sq_sum = sum(x_i^2)` по всему массиву.

По ним вычисляется:

- `mean = global_sum / N`
- `variance = (global_sq_sum / N) - mean * mean`
- `stddev = sqrt(variance)`

После этого `rank = 0` выводит результаты.

## 10. Вывод программы

Программа выводит (только на `rank = 0`):

- среднее значение массива,
- стандартное отклонение массива.
