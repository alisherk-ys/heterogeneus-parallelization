# Задание 3 — MPI: параллельный поиск кратчайших путей (Флойд–Уоршелл)

## 1. Цель задания

Нужно реализовать параллельную MPI-программу для поиска **кратчайших путей между всеми парами вершин** в взвешенном ориентированном графе с помощью алгоритма **Флойда–Уоршелла**.

По условию требуется:

1. Процесс `rank = 0` создаёт матрицу смежности графа `G` размера `N×N`.
2. Разделить строки матрицы между процессами с помощью Scatter (в решении используется `MPI_Scatterv`, чтобы корректно работать при любом количестве процессов).
3. Реализовать алгоритм Флойда–Уоршелла:
   - каждый процесс обновляет свою часть матрицы на текущей итерации,
   - на каждой итерации выполнить обмен данными между процессами через `MPI_Allgather` (в решении используется `MPI_Allgatherv`).
4. После завершения всех итераций собрать итоговую матрицу на `rank = 0` и вывести результат (полностью для маленьких `N`, иначе — выборочные значения).

## 2. Идея решения

Алгоритм Флойда–Уоршелла для матрицы расстояний `dist` выглядит так:

Для `k = 0..N-1`:
- для всех `i, j` выполняем:
  - `dist[i][j] = min(dist[i][j], dist[i][k] + dist[k][j])`

То есть на каждом шаге `k` мы проверяем, можно ли улучшить путь `i → j`, если разрешить проход через промежуточную вершину `k`.

### Как это распараллеливается в MPI

- Матрица `dist` делится по **строкам** между процессами.
- На итерации `k` каждому процессу нужна строка `k` (значения `dist[k][j]` для всех `j`).
- Поэтому после обновления своих строк каждый процесс отправляет свой блок строк всем остальным через `MPI_Allgather`, чтобы у всех появилась актуальная “полная” матрица для следующего шага.

Это и есть ключевая идея задания: **локальное обновление + коллективный обмен** на каждой итерации.

## 3. Подготовка данных на rank 0

Только процесс `rank = 0` создаёт исходный граф.

Граф хранится матрицей `G_full` размера `N×N`:

- `G[i][i] = 0` (расстояние до самой себя),
- если ребра `i→j` нет — значение ставится как “бесконечность” (`INF`),
- если ребро есть — записывается случайный вес `1..maxWeight`.

Для задания плотности графа используется параметр `probEdge` — вероятность существования ребра.

## 4. Распределение строк между процессами

Требование “работает при любом количестве процессов” означает, что `N` может не делиться на `size`.

Поэтому строки делятся так:

- `base = N / size`
- `rem = N % size`
- первые `rem` процессов получают на 1 строку больше.

Используются массивы:

- `counts[p]` — сколько строк у процесса `p`,
- `displs[p]` — с какой глобальной строки начинается блок процесса `p`.

Так как для Scatter/Allgather нужны количества **элементов**, а не строк, формируются:

- `sendcounts[p] = counts[p] * N`
- `senddispls[p] = displs[p] * N`

Распределение выполняется через:
`MPI_Scatterv(...)`

(В условии написано `MPI_Scatter`, но при `N % size != 0` корректнее использовать `MPI_Scatterv`.)

## 5. Локальные и глобальные представления матрицы

После распределения:

- каждый процесс хранит свою часть строк в `G_local` (размер `local_rows × N`),
- дополнительно каждый процесс хранит полную матрицу `G_all` (размер `N × N`), чтобы иметь доступ к строке `k` на каждой итерации.

Сразу после Scatter выполняется один общий сбор:

`MPI_Allgatherv(G_local -> G_all)`

чтобы все процессы имели стартовую версию полной матрицы.

## 6. Параллельный Флойд–Уоршелл

Цикл идёт по `k` от `0` до `N-1`.

На каждом `k`:

1. Берётся строка `k` из общей матрицы:
   - `row_k = &G_all[k * N]`

2. Каждый процесс обновляет только свои строки (локальные `i`):
   - берём `dik = dist[i][k]` из `G_local`,
   - если `dik` “бесконечность” — обновлять строку смысла нет,
   - иначе для каждого `j` вычисляем кандидат:
     - `cand = dik + row_k[j]`
     - и делаем `dist[i][j] = min(dist[i][j], cand)`.

3. После обновления **обязательно выполняется обмен**:
   - `MPI_Allgatherv(G_local -> G_all)`

Это гарантирует, что к следующей итерации `k+1` у всех процессов будет одинаковая актуальная матрица.

## 7. Почему нужен MPI_Allgather

В Флойде–Уоршелле на шаге `k` обновление каждой строки `i` зависит от значений `dist[k][j]`, то есть от строки `k`.

Так как строка `k` может принадлежать любому процессу, требуется, чтобы она была доступна всем.

В данном решении делается максимально прямой вариант:
- на каждой итерации обновлённые блоки строк собираются на всех процессах через `MPI_Allgatherv`,
- поэтому у каждого процесса всегда есть полная матрица `G_all`.

Это соответствует требованию задания использовать `MPI_Allgather` для обмена данными.

## 8. Сбор результатов и вывод

После завершения всех итераций итоговая матрица кратчайших расстояний хранится в `G_all` (на всех процессах).

По заданию вывод делается на `rank = 0`:

- если `N` маленькое — печатается вся матрица,
- если `N` большое — печатать всю матрицу нельзя, поэтому выводятся контрольные значения (несколько элементов/подматрица), чтобы убедиться, что программа отработала.

## 9. Замер времени

Время выполнения алгоритма измеряется через `MPI_Wtime()`:

- старт перед циклом по `k`,
- конец после завершения цикла.

Это время показывает стоимость:
- локальных вычислений,
- плюс коммуникаций `MPI_Allgatherv` на каждой итерации.

## 10. Параметры программы

Размер графа и параметры генерации задаются аргументами командной строки:

- `argv[1]` — `N` (размер графа),
- `argv[2]` — `probEdge` (плотность ребер, 0..1),
- `argv[3]` — `maxWeight` (максимальный вес ребра).

Если аргументы не заданы — используются значения по умолчанию.

## 11. Что выводит программа

Программа выводит (на `rank = 0`):

- сообщение о завершении алгоритма,
- время выполнения,
- матрицу кратчайших путей для маленьких `N` (или часть/контрольные значения для больших `N`).

